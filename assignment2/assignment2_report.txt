TASK 1

Looked at the continous attributes one by one and observed that none of them(except 'age' attribute) are even remotely gaussian. So had to discretize them all. Used the KBinsDiscretizer function from sklearn library for this task.
Then I encoded these categorical attributes using ordinal encoding, with the categories having a higher frequency receiving a lower number.(didnot use onehot encoding because there are just too many attributes)
This dataset has slight class imbalance problem. To overcome the same I did random oversampling.

did log transformation on the prediction variable(y) and then applied decision tree regressor and got an r2 value of 0.84




TASK 2

Bank Marketing Dataset:-

removed the "Duration" attribute to prevent data leakage.
Looked at the continous attributes one by one and observed that none of them(except 'age' attribute) are even remotely gaussian. So had to discretize them all. Used the KBinsDiscretizer function from sklearn library for this task.
Then I encoded these categorical attributes using onehot encoding.
This dataset has severe class imbalance problem. To overcome the same I did random oversampling.
Then applied categorical naivebayes and decision tree algorithms
For this dataset, I chose to use recall score as the preferred performance metric. Because, I want to make sure that I call almost all of the people who are going to subscribe to a term deposit(recall for 'yes' class should be close to 1) and I want to reduce the number of calls to the people who are not going to subscribe to a term deposit(recall for 'no' class should be greater than, say 0.2)
Mathematically, i want recall for 'yes' class to be very close to 1, and a recall for 'no' class that is greater than 0.2 will do. 
So, i am trying to make a VERY permissible classifier
Using NaiveBayes, recall for 'yes' class turns out to be 0.9936 and recall for 'no' class turns out to be 0.26.
Using Decision Trees, recall for 'yes' class turns out to be 0.94 and recall for 'no' class turns out to be 0.14.
Using Random Forests, recall for 'yes' class turns out to be 0.80 and recall for 'no' class turns out to be 0.61.
Using adaboost, recall for 'yes' class turns out to be 0.56 and recall for 'no' class turns out to be 0.90.
clearly, NaiveBayes Model is the best model.





Bollywood Movies Dataset:-

removed the 'Revenue' attribute to prevent data leakage.
Looked at the continous attributes one by one and observed that none of them(except 'age' attribute) are even remotely gaussian. So had to discretize them all. Used the KBinsDiscretizer function from sklearn library for this task.
Then I encoded these categorical attributes using ordinal encoding, with the categories having a higher frequency receiving a lower number.(didnot use onehot encoding because there are just too many attributes)
This dataset has slight class imbalance problem. To overcome the same I did random oversampling.
Then applied categorical naivebayes and decision tree algorithms.
For this dataset,I chose to use f1-score for the class with label 1 as the preferred performance metric. Because, this is an imbalanced dataset(number of hit movies is way less than number of flop movies) and I want to make sure that I predict most of the hit movies as hit(recall for class with label 1), and i predict very few movies as hit if the are actually flop(precision for class with label 1)
Using NaiveBayes, f1 score for class with label 1 turns out to be 0.67
Using Decision Trees, f1 score for class with label 1 turns out to be 0.60
Using  Random Forests, f1 score for class with label 1 turns out to be 0.64
Using Adaboost, f1 score for class with label 1 turns out to be 0.62



Hence Naive bayes has performed better in both datasets.


